# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rg7JPlxkzFhpQYHDT0tW9UCQpIXQMPfz
"""

# --- Import Libraries (think of these as tools we borrow) ---
import os  # For file paths
import streamlit as st  # For web app UI
from dotenv import load_dotenv  # Load secrets from .env
import pandas as pd  # For reading CSV like spreadsheets
from langchain.docstore.document import Document  # To turn data into searchable chunks
from langchain_huggingface import HuggingFaceEmbeddings  # To convert text to numbers for search
from langchain_community.vectorstores import Chroma  # Database for fast search
from langchain_groq import ChatGroq  # Groq AI brain
from langchain.agents import create_react_agent, AgentExecutor  # AI that thinks and uses tools
from langchain.tools import tool  # To define custom tools
from langchain.prompts import PromptTemplate  # Custom instructions for AI
from langchain.memory import ConversationBufferMemory  # Remember chat history
import yfinance as yf  # Fetch stock data

# --- Load Secrets (your Groq key) ---
load_dotenv()  # Reads .env file

# --- Define Custom Tools (functions AI can call) ---
@tool
def get_stock_price(ticker: str):
    """Tool to get current stock price and info. Input: stock ticker like 'AAPL'."""
    try:
        stock = yf.Ticker(ticker)
        info = stock.info
        return f"Current price for {ticker}: ${info.get('currentPrice', 'N/A')}. 52-week high: ${info.get('fiftyTwoWeekHigh', 'N/A')}."
    except:
        return f"Error fetching data for {ticker}."

@tool
def calculate_sharpe_ratio(portfolio_value: float, risk_free_rate: float = 0.02):
    """Simple Sharpe ratio calc. Input: portfolio_value (dummy for now). In real, use returns data."""
    # Dummy example - in real project, fetch historical returns
    returns = 0.15  # Assume 15% annual return
    volatility = 0.20  # Assume 20% volatility
    sharpe = (returns - risk_free_rate) / volatility
    return f"Sharpe ratio: {sharpe:.2f} (higher is better, measures risk-adjusted return)."

# --- Custom Prompt for AI (instructions on how to behave) ---
prompt_template = """
You are a helpful, honest financial advisor. Use tools if needed. Ground answers in user data.
Question: {input}
Thought process: {agent_scratchpad}
"""
prompt = PromptTemplate.from_template(prompt_template)

# --- Streamlit Web App (the UI you see in browser) ---
st.title("Personalized AI Financial Advisor Chatbot")  # Page title

# Sidebar for upload
with st.sidebar:
    st.header("Upload Your Portfolio")
    uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
    if uploaded_file:
        # Read CSV
        df = pd.read_csv(uploaded_file)
        st.success("Portfolio uploaded! Here's a preview:")
        st.dataframe(df.head())  # Show top rows

        # Turn CSV into searchable documents (RAG part)
        docs = []
        for _, row in df.iterrows():
            text = f"Ticker: {row['ticker']}, Shares: {row['shares']}, Purchase Price: ${row['purchase_price']}"
            docs.append(Document(page_content=text, metadata={"source": "portfolio.csv"}))

        # Embed and store in Chroma (like a search index)
        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
        if 'vectorstore' in st.session_state:
            del st.session_state['vectorstore']  # Reset if re-upload
        st.session_state.vectorstore = Chroma.from_documents(docs, embeddings, collection_name="portfolio")

# Main chat area
if "messages" not in st.session_state:
    st.session_state.messages = []  # Chat history
if "agent_executor" not in st.session_state:
    # Set up AI agent only once
    llm = ChatGroq(model="llama-3.1-70b-versatile", temperature=0.7)  # Groq model: smart and fast
    tools = [get_stock_price, calculate_sharpe_ratio]  # Tools AI can use

    # Retriever as a tool (for RAG - search user data)
    @tool
    def retrieve_portfolio(query: str):
        """Search user portfolio for info."""
        if 'vectorstore' not in st.session_state:
            return "No portfolio uploaded yet."
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
        results = retriever.invoke(query)
        return "\n".join([doc.page_content for doc in results])

    tools.append(retrieve_portfolio)  # Add RAG tool

    # Agent setup (thinks step-by-step)
    agent = create_react_agent(llm, tools, prompt)
    memory = ConversationBufferMemory(memory_key="chat_history")  # Remember past messages
    st.session_state.agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True, handle_parsing_errors=True)

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input
if user_input := st.chat_input("Ask about your finances (e.g., 'What's my TSLA holding worth?')"):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # Run AI agent
            response = st.session_state.agent_executor.invoke({"input": user_input})
            st.markdown(response["output"])
            st.session_state.messages.append({"role": "assistant", "content": response["output"]})